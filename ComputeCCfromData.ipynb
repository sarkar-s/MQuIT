{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob #filenames and pathnames utility\n",
    "import os   #operating sytem utility\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random as rand\n",
    "import sys\n",
    "import math\n",
    "from BA_C import BA\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(df,a,b):\n",
    "    return (a*df + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data_directory: folder that contains the csv files for a method and replicate.\n",
    "fileset: Name of the csv files.\n",
    "replicate_name: Part of the csv filename that identifies the replicate number.\n",
    "\"\"\"\n",
    "\n",
    "data_directory = '/Users/sns9/Research/Transcription/SingleCellDistributions_2_CC/Microscopy_HCR_TAMRA_Rep3'\n",
    "fileset = 'Microscopy_HCR_TAMRA_1818.'\n",
    "replicate_name = '_Rep3'\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_set,index_set):\n",
    "    extracted_data = []\n",
    "\n",
    "    for i in index_set:\n",
    "        try:\n",
    "            extracted_data.append(data_set.values[i])\n",
    "        except IndexError:\n",
    "            print(i)\n",
    "            sys.exit()\n",
    "\n",
    "    return np.array(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38774967.0 128952.0\n",
      "bins:  [560, 55, 55, 98, 90, 57, 86, 101]\n"
     ]
    }
   ],
   "source": [
    "response_files = glob.glob('*.csv')\n",
    "\n",
    "glob_max = -1\n",
    "glob_min = 1e12\n",
    "\n",
    "datas = {}\n",
    "conclist = []\n",
    "\n",
    "samples = 1e12\n",
    "\n",
    "for f in response_files:\n",
    "    if fileset in f:\n",
    "        concname = f\n",
    "        conc = concname.replace(fileset,'')\n",
    "        conc = int(conc.replace(replicate_name+'.csv',''))\n",
    "        \n",
    "        if conc not in conclist:\n",
    "            temp_d = pd.read_csv(f).to_numpy()[:,0]\n",
    "            \n",
    "            datas[int(conc)] = temp_d\n",
    "            \n",
    "            glob_max = max(glob_max,max(datas[conc]))\n",
    "            glob_min = min(glob_min,min(datas[conc]))\n",
    "            \n",
    "            conclist.append(conc)\n",
    "            \n",
    "conclist.sort()\n",
    "print(glob_max,glob_min)\n",
    "conclist\n",
    "\n",
    "# Offset to shift all the expression values >= 1\n",
    "offset = -glob_min + 1\n",
    "\n",
    "bins = []\n",
    "\n",
    "for conc in conclist:\n",
    "    datas[conc] += offset\n",
    "    \n",
    "    \"\"\"Get the number of bins for each concentration determined by Freedman-Diaconis rule.\n",
    "    Choice of linear scale (typically for RNAs) or log scale (typically for Proteins). \n",
    "    \"\"\"\n",
    "    \n",
    "    hist, bin_edges = np.histogram(datas[conc],bins='fd')\n",
    "    \n",
    "    bins.append(len(hist))\n",
    "    \n",
    "print('bins: ',bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Freedman-Diaconis ('fd') is only a rule of thumb to estimate the number of bins. \n",
    "It does not provide the exact number of bins required for accurately calculating \n",
    "channel capacity. Start with a value for n_bins roughly 1/10th of the 'fd' estimates, \n",
    "compute the channel capacity, then increase the n_bins and repeat the calculation.\n",
    "\"\"\"\n",
    "bin_set = [10,20,40,80,160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = open('C_summary.csv','w')\n",
    "print('Bins,C',file=of)\n",
    "of.close()\n",
    "\n",
    "try:\n",
    "    os.chdir(data_directory+'/response')\n",
    "except OSError:\n",
    "    os.mkdir(data_directory+'/response')\n",
    "    os.chdir(data_directory+'/response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.026688627310025415\n",
      "20 0.11964272575438128\n",
      "40 0.47104442105709254\n",
      "80 0.735148332728254\n",
      "160 0.773469271085845\n"
     ]
    }
   ],
   "source": [
    "bao = BA()\n",
    "\n",
    "# Number of bins\n",
    "\n",
    "for n_bins in bin_set:\n",
    "    bin_edges = np.linspace(1.0,glob_max + offset,n_bins+1)\n",
    "    \n",
    "    # Compute and write probability transition matrix\n",
    "    pdfs = np.zeros(shape=(len(conclist),n_bins))\n",
    "    for j in range(0,len(conclist)):\n",
    "        hist, bin_edges = np.histogram(datas[conc],bins=bin_edges)\n",
    "        pdfs[j,:] = hist/np.sum(hist)\n",
    "        \n",
    "    np.savetxt('expressions.csv',pdfs,delimiter=',')\n",
    "    \n",
    "    # Compute probability transition matrix for subsampled data with replicates\n",
    "    data_fractions = [1,2,5,10]\n",
    "    n_reps = 5\n",
    "\n",
    "    c_subsample = np.zeros(shape=(n_reps*len(data_fractions),2))\n",
    "    idx = 0\n",
    "\n",
    "    for df in data_fractions:\n",
    "        for k in range(1,6):\n",
    "            pdfs = np.zeros(shape=(len(conclist),n_bins))\n",
    "\n",
    "            for i in range(0,len(conclist)):\n",
    "                cs = conclist[i]\n",
    "                darray = datas[cs]\n",
    "                darray_list = list(darray)\n",
    "                sample_size = int(len(darray_list)/df)\n",
    "\n",
    "                d_sampled = rand.sample(darray_list,sample_size)\n",
    "\n",
    "                hist, b_edges = np.histogram(np.array(d_sampled),bin_edges)\n",
    "\n",
    "                pdfs[i,:] = hist/np.sum(hist)\n",
    "\n",
    "            response_file = 'expressions'+str(int(df))+'_'+str(k)+'.csv'\n",
    "            \n",
    "            if n_bins!=bin_set[-1]:\n",
    "                np.savetxt(response_file,pdfs,delimiter=',')\n",
    "                \n",
    "            \n",
    "            # Compute channel capacity\n",
    "            bao.set_response(pdfs)\n",
    "            c, e, p = bao.get_CC()\n",
    "\n",
    "            c_subsample[idx,0] = float(df)\n",
    "            c_subsample[idx,1] = c\n",
    "            \n",
    "            idx += 1\n",
    "\n",
    "    popt, pcov = curve_fit(linear, c_subsample[:,0], c_subsample[:,1])\n",
    "\n",
    "    os.chdir('..')\n",
    "    print(str(int(n_bins))+','+str(\"{:0.2f}\".format(popt[1])),file=open('C_summary.csv','a'))\n",
    "    os.chdir('response')\n",
    "    \n",
    "    print(n_bins,popt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
